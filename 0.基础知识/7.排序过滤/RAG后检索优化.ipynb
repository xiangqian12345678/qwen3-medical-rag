{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 后检索优化\n",
    "    1.重排序-RRF\n",
    "        重定义class RRFMultiQueryRetriever(MultiQueryRetriever)\n",
    "            多路召回结果不去重\n",
    "            根据多路召回的结果进行RRF排序\n",
    "    2.重排序-CrossEncoderReranker\n",
    "        model = HuggingFaceCrossEncoder(model_name=model_path, model_kwargs={'device': 'cpu'})\n",
    "        compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "        compression_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor,\n",
    "            base_retriever=retriever  # retriever = 混合检索 或 multi-query\n",
    "        )\n",
    "    3.重排序-LongContextReorder\n",
    "        将重要文本放到长文本的开头或结尾，通常会获得最佳性能\n",
    "    4.压缩过滤-LLMChainExtractor\n",
    "      让 LLM 在文档内部做 “抽取式过滤”，把无关内容裁掉，只保留与查询有关的部分\n",
    "        内容过滤\n",
    "        内容压缩\n",
    "        去除无关部分\n",
    "    5.压缩过滤-LLMChainFilter\n",
    "      将文档与查询一起输入给 LLM，由 LLM 判断该文档是否与查询足够相关。如果不相关直接丢弃，不进入后续处理链\n",
    "    6.压缩过滤-EmbeddingsFilter\n",
    "        EmbeddingsFilter 通过计算查询与文档的 embedding 相似度，根据阈值判断是否保留文档\n",
    "        与LLMChainFilter类似，但LLMChainFilter = 让 LLM 做语义判断的“智能过滤器（Keep/Drop）\n",
    "    7.冗余过滤-EmbeddingsRedundantFilter\n",
    "        对于相似度超过阈值的文档对，只保留其中一个\n",
    "        redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings_model, similarity_threshold=0.95)"
   ],
   "id": "449922093b3608ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.冲排序-RRF\n",
    "import logging\n",
    "from typing import List\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.retrievers import MultiQueryRetriever\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_community.embeddings.dashscope import DashScopeEmbeddings\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "# 格式化输出内容\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i + 1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 1. 模型初始化\n",
    "llm = ChatTongyi(model=\"qwen-max\")\n",
    "embeddings_model = DashScopeEmbeddings(model=\"text-embedding-v1\")\n",
    "\n",
    "# 2. 准备数据\n",
    "texts = [\n",
    "    \"人工智能在医疗诊断中的应用。\",\n",
    "    \"人工智能如何提升供应链效率。\",\n",
    "    \"NBA季后赛最新赛况分析。\",\n",
    "    \"传统法式烘焙的五大技巧。\",\n",
    "    \"红楼梦人物关系图谱分析。\",\n",
    "    \"人工智能在金融风险管理中的应用。\",\n",
    "    \"人工智能如何影响未来就业市场。\",\n",
    "    \"人工智能在制造业的应用。\",\n",
    "    \"今天天气怎么样\",\n",
    "    \"人工智能伦理：公平性与透明度。\"\n",
    "]\n",
    "\n",
    "# 为每个文档生成唯一ID和元数据\n",
    "ids = [str(uuid4()) for _ in range(len(texts))]  # 生成UUID作为ID\n",
    "metadatas = [{\"source\": f\"doc_{i + 1}\", \"id\": f\"{ids[i]}\"} for i in range(len(texts))]  # 可选的元数据\n",
    "\n",
    "# 3.向量存储\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings_model,\n",
    "    ids=ids,\n",
    "    metadatas=metadatas,\n",
    "    collection_name=\"rrf\"\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 4. 多query查询召回器重构\n",
    "# 重写MultiQueryRetriever类，取消unique_union去重，且保留每个问题检索结果的\n",
    "class RRFMultiQueryRetriever(MultiQueryRetriever):\n",
    "    # 改写retrieve_documents方法，返回rrf结果\n",
    "    def retrieve_documents(\n",
    "            self, queries: List[str], run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        documents = []\n",
    "        for query in queries:\n",
    "            docs = self.retriever.invoke(\n",
    "                query, config={\"callbacks\": run_manager.get_child()}\n",
    "            )\n",
    "            # 原代码中extend修改为append，保持不同检索系统的结构\n",
    "            documents.append(docs)\n",
    "\n",
    "        documents = self.rrf_documents(documents)\n",
    "        return documents\n",
    "\n",
    "    def rrf_documents(self, documents: list[list[Document]], k=60) -> List[Document]:\n",
    "        # 初始化rrf字典（key=文档id，value={\"rrf_score\":累计分数,\"doc\":文档对象}）\n",
    "        rrf_scores = {}\n",
    "        # 遍历每个检索结果列表（每个查询对应的结果）\n",
    "        for docs in documents:\n",
    "            # 为每个文档列表计算排名（从1开始）\n",
    "            for rank, doc in enumerate(docs, 1):\n",
    "                # 计算当前文档的RRF分数\n",
    "                rrf_score = 1 / (k + rank)\n",
    "                # 如果文档已经在字典中，累加RRF分数\n",
    "                if doc.metadata.get(\"id\") in rrf_scores:\n",
    "                    rrf_scores[doc.metadata.get(\"id\")]['rrf_score'] += rrf_score\n",
    "                else:\n",
    "                    rrf_scores[doc.metadata.get(\"id\")] = {'rrf_score': rrf_score, 'doc': doc}\n",
    "\n",
    "        # 将字典转换为列表，并根据字段value：RRF分数排序\n",
    "        sorted_docs = sorted(\n",
    "            rrf_scores.values(),\n",
    "            key=lambda x: x['rrf_score'],\n",
    "            reverse=True  # 降序排列：从大到小\n",
    "        )\n",
    "\n",
    "        result = [item['doc'] for item in sorted_docs]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# 5. 检索\n",
    "rrf_retriever = RRFMultiQueryRetriever.from_llm(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    include_original=True  # 是否包含原始查询\n",
    ")\n",
    "\n",
    "rrf_docs = rrf_retriever.invoke(\"人工智能的应用\")\n",
    "\n",
    "pretty_print_docs(rrf_docs)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2.CrossEncoderReranker+ContextualCompressionRetriever\n",
    "import os\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "\n",
    "\"\"\"\n",
    "1. CrossEncoderReranker 初始化\n",
    "    compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "    作用：创建一个基于交叉编码器(Cross-Encoder)的重排序器\n",
    "    参数解析：\n",
    "        model: 使用的重排序模型实例(这里是HuggingFaceCrossEncoder)\n",
    "        top_n: 指定保留前多少个重排序后的结果(这里设为3)\n",
    "\n",
    "    工作原理：\n",
    "        接收初始检索结果(通常来自向量检索)\n",
    "        对每个文档与查询(query)的相关性进行精细评分\n",
    "        根据评分重新排序文档\n",
    "        只保留top_n个最相关的文档\n",
    "\n",
    "2. ContextualCompressionRetriever 创建\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=retriever\n",
    "    )\n",
    "    作用：将基础检索器和重排序器组合成一个压缩检索器\n",
    "    参数解析：\n",
    "        base_compressor: 上面创建的CrossEncoderReranker实例\n",
    "        base_retriever: 基础检索器(这里是Chroma向量检索器)\n",
    "\n",
    "工作流程：\n",
    "    首先使用base_retriever获取初步检索结果\n",
    "    然后使用base_compressor对这些结果进行重排序和过滤\n",
    "\n",
    "3. 检索过程\n",
    "    compressed_docs = compression_retriever.invoke(\"人工智能的应用\")\n",
    "    执行流程：\n",
    "        向量检索器(retriever)首先找到与\"人工智能的应用\"相关的文档(基于向量相似度)\n",
    "        重排序器(compressor)对这些文档进行更精细的相关性评估：\n",
    "        计算查询与每个文档的交叉注意力\n",
    "        生成更准确的相关性分数\n",
    "        根据重排序分数，只保留前3个最相关的文档\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 格式化输出内容\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i + 1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# 1. 数据准备\n",
    "texts = [\n",
    "    \"人工智能在医疗诊断中的应用。\",\n",
    "    \"人工智能如何提升供应链效率。\",\n",
    "    \"NBA季后赛最新赛况分析。\",\n",
    "    \"传统法式烘焙的五大技巧。\",\n",
    "    \"红楼梦人物关系图谱分析。\",\n",
    "    \"人工智能在金融风险管理中的应用。\",\n",
    "    \"人工智能如何影响未来就业市场。\",\n",
    "    \"人工智能在制造业的应用。\",\n",
    "    \"今天天气怎么样\",\n",
    "    \"人工智能伦理：公平性与透明度。\"\n",
    "]\n",
    "\n",
    "# 2.文档向量化并存储\n",
    "embeddings_model = DashScopeEmbeddings(model=\"text-embedding-v1\")\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings_model,\n",
    "    collection_name=\"rrf\"\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 3.排序模型\n",
    "MODEL_DIR = \"/mnt/c/大模型/智泊大模型全栈教程总结/02-教材整理 L2/代码/Langchain/data/BAAI\"\n",
    "model_name = \"bge-reranker-large\"\n",
    "model_path = os.path.join(MODEL_DIR, model_name)\n",
    "model = HuggingFaceCrossEncoder(model_name=model_path, model_kwargs={'device': 'cpu'})\n",
    "\n",
    "# 4.重排序模型初始化\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever  # retriever = 混合检索 或 multi-query\n",
    ")\n",
    "\n",
    "# 5.召回并重排序\n",
    "compressed_docs = compression_retriever.invoke(\"人工智能的应用\")\n",
    "pretty_print_docs(compressed_docs)\n"
   ],
   "id": "2dbeb77b2ec87de6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3.LongContextReorder\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "\"\"\"\n",
    "根据论文（Lost in the Middle: How Language Models Use Long Contexts）当关键数据位于输入上下文的开头或结尾时，\n",
    "通常会获得最佳性能。为了减轻 “lost in the middle”的影响，可以在检索后重新排序文档，使最相关的文档置于极值\n",
    "（例如，上下文的第一和最后一部分），将最不相关的文档置于中间。\n",
    "\"\"\"\n",
    "# 5,4,3,2,1\n",
    "# 倒排：1,2,3,4,5\n",
    "# index%2=0: 往第一个放，index%2=1 往最后放\n",
    "\n",
    "documents = [\n",
    "    \"相关性:5\",\n",
    "    \"相关性:4\",\n",
    "    \"相关性:3\",\n",
    "    \"相关性:2\",\n",
    "    \"相关性:1\",\n",
    "]\n",
    "\n",
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(documents)\n",
    "\n",
    "print(reordered_docs)\n"
   ],
   "id": "b17d0259a722c365"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4.LLMChainExtractor\n",
    "import os\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\"\"\"\n",
    "文档分块检索后，与查询最相关的信息可能隐藏在一个包含大量不相关文本的文档中，输入给LLM，可能会导致更昂贵的LLM调用和较差的响应（噪声）。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i + 1}:\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# 1.数据准备\n",
    "RESOURCE_DIR = \"/mnt/c/大模型/智泊大模型全栈教程总结/02-教材整理 L2/代码/Langchain/6.langchain高级RAG/data/resources\"\n",
    "TXT_DOCUMENT_PATH = os.path.join(RESOURCE_DIR, \"deepseek百度百科.txt\")\n",
    "loader = TextLoader(TXT_DOCUMENT_PATH, encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "# 2.向量化并索引\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "embeddings_model = DashScopeEmbeddings(model=\"text-embedding-v1\")\n",
    "retriever = Chroma.from_documents(chunks, embeddings_model).as_retriever()\n",
    "\n",
    "# 3.测试召回\n",
    "docs = retriever.invoke(\"deepseek的发展历程\")\n",
    "\n",
    "print(\"\\n1. 压缩前\" + '-' * 100)\n",
    "pretty_print_docs(docs)\n",
    "\n",
    "# 4.压缩过滤\n",
    "llm = ChatTongyi(model=\"qwen-max\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "compressed_docs = compression_retriever.invoke(\"deepseek的发展历程\")\n",
    "\n",
    "print(\"\\n2. 压缩后\" + '-' * 100)\n",
    "pretty_print_docs(compressed_docs)\n"
   ],
   "id": "a21aa3eb5c211d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 5.压缩过滤-LLMChainFilter\n",
    "import os\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i + 1}:\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# 1.数据准备\n",
    "RESOURCE_DIR = \"/mnt/c/大模型/智泊大模型全栈教程总结/02-教材整理 L2/代码/Langchain/6.langchain高级RAG/data/resources\"\n",
    "TXT_DOCUMENT_PATH = os.path.join(RESOURCE_DIR, \"deepseek百度百科.txt\")\n",
    "loader = TextLoader(TXT_DOCUMENT_PATH, encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "# 2.向量化并索引\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "embeddings_model = DashScopeEmbeddings(model=\"text-embedding-v1\")\n",
    "retriever = Chroma.from_documents(chunks, embeddings_model).as_retriever()\n",
    "\n",
    "# 3.测试召回\n",
    "docs = retriever.invoke(\"deepseek的发展历程\")\n",
    "print(\"\\n1.向量召回\" + '-' * 100)\n",
    "pretty_print_docs(docs)\n",
    "\n",
    "# 4. 过滤器\n",
    "'''\n",
    "Given the following question and context, return YES if the context is relevant to the question and NO if it isn't.\n",
    "> Question: {question}\n",
    "> Context:\n",
    ">>>\n",
    "{context}\n",
    ">>>\n",
    "> Relevant (YES / NO):\n",
    "\n",
    "给定以下问题和上下文，如果上下文与问题相关，则返回YES，否则返回NO。\n",
    "问题：{question}\n",
    "上下文：\n",
    ">>>\n",
    "{context}\n",
    ">>>\n",
    "相关性（是/否）：\n",
    "'''\n",
    "llm = ChatTongyi(model=\"qwen-max\")\n",
    "_filter = LLMChainFilter.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "# 5. 过滤召回\n",
    "compressed_docs = compression_retriever.invoke(\"deepseek的发展历程\")\n",
    "\n",
    "print(\"\\n2.LLMChainFilter过滤后\" + '-' * 100)\n",
    "pretty_print_docs(compressed_docs)\n"
   ],
   "id": "27850d1189ab9a56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 6.压缩过滤-EmbeddingsFilter\n",
    "import os\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i + 1}:\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# 1.数据准备\n",
    "RESOURCE_DIR = \"/mnt/c/大模型/智泊大模型全栈教程总结/02-教材整理 L2/代码/Langchain/6.langchain高级RAG/data/resources\"\n",
    "TXT_DOCUMENT_PATH = os.path.join(RESOURCE_DIR, \"deepseek百度百科.txt\")\n",
    "loader = TextLoader(TXT_DOCUMENT_PATH, encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "# 2.向量化并索引\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "embeddings_model = DashScopeEmbeddings(model=\"text-embedding-v1\")\n",
    "retriever = Chroma.from_documents(chunks, embeddings_model).as_retriever()\n",
    "\n",
    "# 3.测试召回\n",
    "docs = retriever.invoke(\"deepseek的发展历程\")\n",
    "print(\"\\n1.向量召回\" + '-' * 100)\n",
    "pretty_print_docs(docs)\n",
    "\n",
    "# 4. 过滤器\n",
    "# 对检索到的文档块与查询进行相似度计算，如果相似度大于0.66，则保留该文档块，否则过滤掉\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings_model, similarity_threshold=0.66)\n",
    "\n",
    "# 5. 过滤召回\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\"deepseek的发展历程\")\n",
    "\n",
    "print(\"\\n2.过滤后\" + '-' * 100)\n",
    "pretty_print_docs(compressed_docs)\n",
    "\n"
   ],
   "id": "348376b35cd61eb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 7.冗余过滤\n",
    "import os\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import EmbeddingsFilter, DocumentCompressorPipeline\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i + 1}:\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# 1.数据准备\n",
    "RESOURCE_DIR = \"/mnt/c/大模型/智泊大模型全栈教程总结/02-教材整理 L2/代码/Langchain/6.langchain高级RAG/data/resources\"\n",
    "TXT_DOCUMENT_PATH = os.path.join(RESOURCE_DIR, \"deepseek百度百科.txt\")\n",
    "loader = TextLoader(TXT_DOCUMENT_PATH, encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "# 2.向量化并索引\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "embeddings_model = DashScopeEmbeddings(model=\"text-embedding-v1\")\n",
    "retriever = Chroma.from_documents(chunks, embeddings_model).as_retriever()\n",
    "\n",
    "# 3.测试召回\n",
    "docs = retriever.invoke(\"deepseek的发展历程\")\n",
    "print(\"\\n1.向量召回\" + '-' * 100)\n",
    "pretty_print_docs(docs)\n",
    "\n",
    "# 4. 过滤器\n",
    "# 默认文档间相似度超过0.95则为冗余文档\n",
    "# 计算所有文档之间的嵌入向量相似度\n",
    "# 对于相似度超过阈值的文档对，只保留其中一个\n",
    "# 直接使用  redundant_filter.transform_documents(documents)\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings_model, similarity_threshold=0.95)\n",
    "# 根据问题与文档的相似度过滤\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings_model, similarity_threshold=0.66)\n",
    "\n",
    "# 首先应用redundant_filter去除冗余文档\n",
    "# 然后应用relevant_filter去除与查询不相关的文档\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[redundant_filter, relevant_filter]\n",
    ")\n",
    "\n",
    "# 5. 过滤召回\n",
    "# 首先通过基础检索器(Chroma)获取初步检索结果\n",
    "# 然后通过压缩管道对结果进行过滤和优化\n",
    "# 最终返回精炼后的、高质量的文档集\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor, base_retriever=retriever\n",
    ")\n",
    "compressed_docs = compression_retriever.invoke(\"deepseek的发展历程\")\n",
    "\n",
    "print(\"\\n2.过滤后\" + '-' * 100)\n",
    "pretty_print_docs(compressed_docs)\n",
    "\n",
    "\n"
   ],
   "id": "d51879ff472cd19e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
