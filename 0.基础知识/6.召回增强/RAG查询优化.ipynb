{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 查询优化\n",
    "    1.完善问题\n",
    "        不断引导用户完善访问信息\n",
    "    2.多路召回 MultiQueryRetriever\n",
    "        将原问题生成多个问题，然后用这些问题去向量检索\n",
    "        相当于：一次查询 → 多次检索 → 合并结果 → 去重\n",
    "    3.问题分解\n",
    "        将问题拆解为子问题\n",
    "        根据子问题进行文档检索\n",
    "        根据检索到的文档回答原问题\n",
    "    4.上位优化\n",
    "        生成更抽象的上位问题\n",
    "        结合上位问题和原问题获取更准确答案\n",
    "    5.假设性文档嵌入\n",
    "        根据文档生成假设性答案\n",
    "        根据假设答案向量去向量检索\n",
    "        根据检索到的文档回答原问题\n",
    "    6.混合检索\n",
    "        基于向量检索文档\n",
    "        基于关键词检索文档\n",
    "        融合检索结果  EnsembleRetriever(retrievers=[BM25_retriever, vector_retriever], weights=[0.5, 0.5])\n",
    "        根据混合检索结果回答问题"
   ],
   "id": "a88ae8480defc858"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.完善问题\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_community.embeddings.dashscope import DashScopeEmbeddings\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# 1.模型初始化\n",
    "llm = ChatTongyi(model=\"qwen-max\")\n",
    "embeddings_model = DashScopeEmbeddings(model=\"text-embedding-v1\")\n",
    "\n",
    "\n",
    "# 格式化输出内容\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i + 1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 2.意图识别\n",
    "# 2.1 示例业务模板\n",
    "templates = {\n",
    "    \"订机票\": [\"起点\", \"终点\", \"时间\", \"座位等级\", \"座位偏好\"],\n",
    "    \"订酒店\": [\"城市\", \"入住日期\", \"退房日期\", \"房型\", \"人数\"],\n",
    "}\n",
    "\n",
    "# 2.2 意图识别提示模板\n",
    "intent_prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\", \"templates\"],\n",
    "    template=\"根据用户输入 '{user_input}'，选择最合适的业务模板。可用模板如下：{templates}。请返回模板名称。\"\n",
    ")\n",
    "\n",
    "# 2.3 创建意图识别链\n",
    "intent_chain = intent_prompt | llm\n",
    "\n",
    "# 2.4 模拟用户输入\n",
    "user_input = \"我想订一张长沙去北京的机票\"\n",
    "\n",
    "# 2.5 识别意图\n",
    "intent = intent_chain.invoke({\"user_input\": user_input, \"templates\": str(list(templates.keys()))}).content\n",
    "print(\"意图：\", intent)\n",
    "\n",
    "# 3.优化访问模版\n",
    "# 3.1 优化模版构建\n",
    "selected_template = templates.get(intent)\n",
    "print(\"模板：\", selected_template)\n",
    "\n",
    "# 补充信息提示模板\n",
    "info_prompt = f\"\"\"\n",
    "    请根据用户原始问题和模板，判断原始问题是否完善。\n",
    "    如果问题缺乏需要的信息，请生成一个友好的请求，明确指出需要补充的信息。\n",
    "    若问题完善后，返回包含所有信息的完整问题。\n",
    "\n",
    "    ### 原始问题\n",
    "    {user_input}\n",
    "\n",
    "    ### 模板\n",
    "    {\",\".join(selected_template)}\n",
    "\n",
    "    ### 输出示例\n",
    "    {{\n",
    "        \"isComplete\": true,\n",
    "        \"content\": \"`完整问题`\"\n",
    "    }}\n",
    "    {{\n",
    "        \"isComplete\": false,\n",
    "        \"content\": \"`友好的引导到需要补充信息`\"\n",
    "    }}\n",
    "\"\"\"\n",
    "\n",
    "print(f\"info_prompt: \\n {info_prompt}\")\n",
    "\n",
    "# 3.2 基于优化模版访问\n",
    "# 历史记录\n",
    "chat_history = ChatMessageHistory()\n",
    "# 聊天模版\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"你是一个信息补充助手，任务是分析用户问题是否完整。\"),\n",
    "        (\"placeholder\", \"{history}\"),  # 历史记录的占位\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 补充信息链\n",
    "info_chain = prompt | llm\n",
    "\n",
    "# 自动处理历史记录，将记录注入输入并在每次调用后更新它\n",
    "# 1. 系统会根据当前会话 ID 获取聊天历史（通过提供的 lambda 函数）\n",
    "# 2. 将当前输入消息（从 input 键获取）和聊天历史（放入 history 键）组合成最终输入\n",
    "# 3. 调用 info_chain 进行处理\n",
    "# 4. 通常还会将新的交互记录自动保存到聊天历史中\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    info_chain,  # 被包装的链式处理对象（通常是一个 LangChain 链或类似的可运行对象）\n",
    "    lambda session_id: chat_history,  # 消息历史存储的获取函数： 以 session_id 为参数，返回对应的 chat_history 对象\n",
    "    input_messages_key=\"input\",  # 指定输入消息在输入字典中的键名\n",
    "    history_messages_key=\"history\",  # 指定历史消息在输入字典中的键名\n",
    ")\n",
    "\n",
    "# 判断问题是否完整，如果不完整，则生成追问请求 调用被包装的链（info_chain），并自动处理消息历史的注入和管理。\n",
    "# 1. input 键：对应 RunnableWithMessageHistory 初始化时设置的 input_messages_key=\"input\"，表示当前用户消息的字段名\n",
    "# 2. info_prompt：用户的实际输入内容（通常是一个问题或指令字符串）\n",
    "# 3. session_id：标识对话会话的唯一键，用于从历史存储中获取或更新对话历史，在多用户场景中，通过不同 session_id 隔离各自的对话历史\n",
    "info_request = with_message_history.invoke(input={\"input\": info_prompt},\n",
    "                                           config={\"configurable\": {\"session_id\": \"unused\"}}).content\n",
    "\n",
    "# 3.3 解析访问结果\n",
    "parser = JsonOutputParser()\n",
    "json_data = parser.parse(info_request)\n",
    "# 循环判断是否完整，并提交用户补充信息\n",
    "while json_data['isComplete'] is False:\n",
    "    # 根据大模型的引导，用户补充信息\n",
    "    user_answer = input(json_data['content'])\n",
    "    # 提交用户补充信息，并判断问题是否完整\n",
    "    info_request = with_message_history.invoke(input={\"input\": user_answer},\n",
    "                                               config={\"configurable\": {\"session_id\": \"unused\"}}).content\n",
    "\n",
    "    # 打印完整历史记录 确认是否有存储\n",
    "    print(\"=\" * 100)\n",
    "    print(\"当前对话历史：\")\n",
    "    for message in chat_history.messages:\n",
    "        print(f\"{message.type}: {message.content}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    try:\n",
    "        json_data = parser.parse(info_request)\n",
    "    except Exception as e:\n",
    "        print(\"json parse error\")\n",
    "        break\n",
    "print(info_request)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2.多路召回 MultiQueryRetriever\n",
    "# 相当于一次查询 → 多次检索 → 合并结果 → 去重\n",
    "# 它让 LLM 把原始 query 改写为一组语义不同但相关的问题：\n",
    "#   比如 LLM 生成：\n",
    "#       deepseek 遭遇了哪些质疑？\n",
    "#       哪些国家限制或封禁了 deepseek？\n",
    "#       deepseek 在国际上面临哪些挑战？\n",
    "#       deepseek 是否遇到安全类指控？\n",
    "#   然后 用这4个问题全部去向量检索\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.retrievers import MultiQueryRetriever\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.dashscope import DashScopeEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 日志句柄初始化\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# 格式化输出内容\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i + 1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# 1.文件路径\n",
    "RESOURCE_DIR = \"/mnt/c/大模型/智泊大模型全栈教程总结/02-教材整理 L2/代码/Langchain/6.langchain高级RAG/data/resources\"\n",
    "TXT_DOCUMENT_PATH = os.path.join(RESOURCE_DIR, \"deepseek百度百科.txt\")\n",
    "\n",
    "# 2.模型初始化\n",
    "llm = ChatTongyi(model=\"qwen-max\")\n",
    "embeddings_model = DashScopeEmbeddings(model=\"text-embedding-v1\")\n",
    "\n",
    "# 3.加载文档\n",
    "loader = TextLoader(TXT_DOCUMENT_PATH, encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "# 4.召回引擎创建与数据索引\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=chunks,\n",
    "                                    embedding=embeddings_model,\n",
    "                                    collection_name=\"multi-query\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 5. 多路召回\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "        You are an AI language model assistant.\n",
    "        Your task is to generate 5 different versions of the given user\n",
    "        question to retrieve relevant documents from a vector  database.\n",
    "        By generating multiple perspectives on the user question,\n",
    "        your goal is to help the user overcome some of the limitations\n",
    "        of distance-based similarity search.\n",
    "        Provide these alternative questions separated by newlines.\n",
    "        Original question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "retrieval_from_llm = MultiQueryRetriever.from_llm(\n",
    "    prompt=QUERY_PROMPT,\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    include_original=True  # 是否包含原始查询\n",
    ")\n",
    "\n",
    "# 自动去重\n",
    "unique_docs = retrieval_from_llm.invoke(\"详细介绍DeepSeek\")\n",
    "pretty_print_docs(unique_docs)\n",
    "\n",
    "# 6. 答案合成\n",
    "# 创建prompt模板\n",
    "template = \"\"\"\n",
    "请根据以下文档回答问题:\n",
    "### 文档:\n",
    "{context}\n",
    "### 问题:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# 由模板生成prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"context\": [doc.page_content for doc in unique_docs], \"question\": \"详细介绍DeepSeek\"})\n",
    "print('-'*20 + \"答案合成\" + '-'*20)\n",
    "print(response.content)\n"
   ],
   "id": "c32f1a713e5c2d6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3.问题分解\n",
    "from typing import List\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.retrievers.multi_query import LineListOutputParser\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_community.embeddings.dashscope import DashScopeEmbeddings\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate, BasePromptTemplate\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import Runnable, RunnableLambda\n",
    "\n",
    "\n",
    "# 格式化输出内容\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i + 1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# 1. 模型初始化\n",
    "llm = ChatTongyi(model=\"qwen-max\")\n",
    "embeddings_model = DashScopeEmbeddings(model=\"text-embedding-v1\")\n",
    "\n",
    "# 2. 数据准备\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"番茄炒蛋的食材：\\n\n",
    "        - 新鲜鸡蛋：3-4个（根据人数调整）\n",
    "        - 番茄：2-3个中等大小\\n- 盐：适量\n",
    "        - 白糖：一小勺（可选，用于提鲜）\n",
    "        - 食用油：适量\n",
    "        - 葱花：少许（可选，用于增香）\\n\n",
    "        这些是最基本的材料，当然也可以根据个人口味添加其他调料或配料。\n",
    "        \"\"\"),\n",
    "    Document(\n",
    "        page_content=\"\"\"番茄炒蛋的步骤：鸡蛋打入碗中，加入少许盐，用筷子或打蛋器充分搅拌均匀；\n",
    "           - 番茄洗净后切成小块备用。\\n\n",
    "           3. **炒鸡蛋**：锅内倒入适量食用油加热至温热状态，然后将搅拌好的鸡蛋液缓缓倒入锅中。\n",
    "           待鸡蛋凝固时轻轻翻动几下，让其受热均匀直至完全熟透，随后盛出备用。\\n\n",
    "           4. **炒番茄**：在同一锅里留下的底油中放入切好的番茄块，中小火慢慢翻炒至出汁，可根据个人口味加一点点白糖提鲜。\\n\n",
    "           5. **合炒**：当番茄炒至软烂并开始释放大量汤汁时，再把之前炒好的鸡蛋倒回锅里，快速与番茄混合均匀，同时加入适量的盐调味。\n",
    "           如果喜欢的话还可以撒上一些葱花增加香气。\\n\n",
    "           6. **完成**：最后检查一下味道是否合适，确认无误后即可关火装盘享用美味的番茄炒蛋啦！\n",
    "           \"\"\"),\n",
    "    Document(\n",
    "        page_content=\"\"\"技巧与注意事项：\n",
    "        1. **选材**：选择新鲜的鸡蛋和成熟的番茄。新鲜的食材是做好这道菜的基础。\n",
    "        2. **打蛋液**：将鸡蛋打入碗中后加入少许盐（根据个人口味调整），然后充分搅拌均匀。这样做可以让蛋更加松软且味道更佳。\n",
    "        3. **处理番茄**：番茄最好先用开水稍微焯一下皮，然后去皮切块。这样可以去除表皮的硬质部分，让番茄更容易入味，并且口感更好。\n",
    "        4. **热锅冷油**：先用中小火把锅烧热，再倒入适量食用油，待油温五成热时下蛋液。这样的做法可以使蛋快速凝固形成漂亮的形状而不易粘锅。\n",
    "        5. **分步烹饪**：通常建议先炒鸡蛋至半熟状态取出备用；接着利用剩下的底油继续翻炒番茄至出汁，\n",
    "        最后再将之前炒好的鸡蛋倒回锅里与番茄混合均匀加热即可。\n",
    "        6. **调味品**：除了基本的盐之外，还可以根据喜好添加少量糖来提鲜或者一点酱油增色添香。注意调味料不宜过多以免掩盖了食材本身的味道。\n",
    "        7. **出锅前加葱花**：如果喜欢的话，在即将完成时撒上一些葱花不仅能增加菜品色泽还能增添香气。\n",
    "        \"\"\")\n",
    "]\n",
    "\n",
    "# 3. 数据向量化存储\n",
    "vectorstore = Chroma.from_documents(documents=documents,\n",
    "                                    embedding=embeddings_model,\n",
    "                                    collection_name=\"decomposition\")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "pretty_print_docs(retriever.invoke(\"番茄炒蛋怎么制作？\"))\n",
    "\n",
    "#  4. 问题分解\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to break down the input question into 3 sub-questions,\n",
    "     and solve the complete problem by solving these sub-questions one by one.\n",
    "     The sub-questions need to retrieve relevant documents in the vector database.\n",
    "     By decomposing the user's question to generate sub-questions,\n",
    "     your goal is to help users overcome some limitations of distance-based similarity search.\n",
    "     Provide these sub-questions separated by newlines, no additional content is required. Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "chain = QUERY_PROMPT | llm | LineListOutputParser()\n",
    "questions = chain.invoke({\"question\": \"番茄炒蛋怎么制作？\"})\n",
    "print('-' * 20 + '分解问题' + '-' * 20)\n",
    "print(questions)\n",
    "\n",
    "# 5. 问题分解整合功能类构建\n",
    "SUB_QUESTION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\", \"sub_question\", \"documents\"],\n",
    "    template=\"\"\"\n",
    "    To address the main problem {question},\n",
    "    you need to first resolve the sub-question {sub_question}.\n",
    "    Below is the reference document provided to support your reasoning:\\n\\n{documents}\\n\\n\n",
    "    Please provide the answer to the current sub-question directly.\"\"\",\n",
    ")\n",
    "\n",
    "class DecompositionQueryRetriever(BaseRetriever):\n",
    "    # 向量数据库检索器\n",
    "    retriever: BaseRetriever\n",
    "    # 生成子问题链\n",
    "    llm_chain: Runnable\n",
    "    # 解决子问题链\n",
    "    sub_llm_chain: Runnable\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "            cls,\n",
    "            retriever: BaseRetriever,\n",
    "            llm: BaseLanguageModel,\n",
    "            prompt: BasePromptTemplate = QUERY_PROMPT,\n",
    "            sub_prompt: BasePromptTemplate = SUB_QUESTION_PROMPT\n",
    "    ) -> \"DecompositionQueryRetriever\":\n",
    "        output_parser = LineListOutputParser()\n",
    "        llm_chain = prompt | llm | output_parser\n",
    "        sub_llm_chain = sub_prompt | llm\n",
    "        return cls(\n",
    "            retriever=retriever,\n",
    "            llm_chain=llm_chain,\n",
    "            sub_llm_chain=sub_llm_chain\n",
    "        )\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "            self,\n",
    "            query: str,\n",
    "            *,\n",
    "            run_manager: CallbackManagerForRetrieverRun,\n",
    "    ) -> List[Document]:\n",
    "        # 生成子问题\n",
    "        sub_queries = self.generate_queries(query)\n",
    "        # 解决子问题\n",
    "        documents = self.retrieve_documents(query, sub_queries)\n",
    "        return documents\n",
    "\n",
    "    def generate_queries(self, question: str) -> List[str]:\n",
    "        response = self.llm_chain.invoke({\"question\": question})\n",
    "        lines = response\n",
    "        print(f\"Generated queries: {lines}\")\n",
    "        return lines\n",
    "\n",
    "    def retrieve_documents(self, query: str, sub_queries: List[str]) -> List[Document]:\n",
    "        sub_llm_chain = RunnableLambda(\n",
    "            # 传入子问题，检索文档并回答\n",
    "            lambda sub_query: self.sub_llm_chain.invoke(\n",
    "                {\n",
    "                    \"question\": query,\n",
    "                    \"sub_question\": sub_query,\n",
    "                    \"documents\": [doc.page_content for doc in self.retriever.invoke(sub_query)]\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        # 批量执行所有的子问题\n",
    "        responses = sub_llm_chain.batch(sub_queries)\n",
    "        # 将子问题和答案合并作为解决主问题的文档\n",
    "        documents = [\n",
    "            Document(page_content=sub_query + \"\\n\" + response.content)\n",
    "            for sub_query, response in zip(sub_queries, responses)\n",
    "        ]\n",
    "        return documents\n",
    "\n",
    "\n",
    "# 6. LangChain问题分解整合测试\n",
    "decompositionQueryRetriever = DecompositionQueryRetriever.from_llm(llm=llm, retriever=retriever)\n",
    "decomposition_docs = decompositionQueryRetriever.invoke(\"番茄炒蛋怎么制作？\")\n",
    "pretty_print_docs(decomposition_docs)\n",
    "\n",
    "# 8. 根据召回的文档解答问题\n",
    "# 创建prompt模板\n",
    "template = \"\"\"\n",
    "    请根据以下文档回答问题:\n",
    "    ### 文档:\n",
    "    {context}\n",
    "    ### 问题:\n",
    "    {question}\n",
    "\"\"\"\n",
    "# 由模板生成prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"context\": [doc.page_content for doc in decomposition_docs], \"question\": \"番茄炒蛋怎么制作？\"})\n",
    "print('-' * 20 + '大模型回答' + '-' * 20)\n",
    "print(response.content)"
   ],
   "id": "5efaf8f3287ec74c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4.上位问题\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "\n",
    "\"\"\"\n",
    "核心思想\n",
    "    Step Back 是一种通过让模型先回答更抽象的“上位问题”（step-back question），再基于抽象答案解决原问题的技术。\n",
    "    其灵感来源于人类思考复杂问题时，会先退一步思考更通用的原则。\n",
    "\n",
    "关键步骤\n",
    "1. 生成上位问题：\n",
    "    从原始问题中提取一个更抽象、更本质的问题。\n",
    "    示例：\n",
    "    原问题 → \"AlphaGo 如何击败李世石？\"\n",
    "    上位问题 → \"强化学习在棋类游戏中的基本原理是什么？\"\n",
    "\n",
    "2. 回答上位问题：\n",
    "    先获取抽象问题的答案（通用知识）。\n",
    "\n",
    "3. 结合解决原问题：\n",
    "    用抽象答案作为上下文，推导出原问题的具体答案。\n",
    "\"\"\"\n",
    "# 1.模型初始化\n",
    "llm = ChatTongyi(model=\"qwen-max\")\n",
    "\n",
    "# 2.上位问题模版\n",
    "step_back_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    基于以下问题，生成一个更抽象的上位问题：\n",
    "    原始问题: {original_question}\n",
    "    上位问题:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# 3. 生成上位问题\n",
    "question = \"AlphaGo 如何击败李世石？\"\n",
    "abstract_answer = llm.invoke(step_back_prompt.format(original_question=question))\n",
    "print(f\"上位问题：\\n{abstract_answer}\")\n",
    "\n",
    "# 4.结合两者回答原问题\n",
    "final_prompt = f\"\"\"请基于以下信息回答问题：\n",
    "    上位问题: {abstract_answer}\n",
    "    原始问题: {question}\n",
    "    最终答案:\"\"\"\n",
    "result = llm.invoke(final_prompt)\n",
    "print('-' * 20 + '结合上位问题和原问题回答' + '-' * 20)\n",
    "print(f\"final_prompt:\\n {final_prompt}\")\n",
    "print('答案：\\n' + result)"
   ],
   "id": "b80f88987144c556"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 5.假设性文档嵌入\n",
    "import os\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.dashscope import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\"\"\"\n",
    "核心思想\n",
    "HyDE（Hypothetical Document Embeddings 假设性文档嵌入） 是一种无需真实文档的检索增强生成（RAG）技术。\n",
    "其核心是通过模型先生成假设性答案，再根据该假设答案的嵌入向量去检索真实文档。\n",
    "\n",
    "关键步骤\n",
    "    1. 生成假设答案：\n",
    "        让模型基于问题生成一个假设的答案（无需准确，只需语义相关）。\n",
    "        示例：\n",
    "        问题 → \"如何训练一只猫用马桶？\"\n",
    "        假设答案 → \"训练猫用马桶需要逐步引导，首先将猫砂盆靠近马桶...\"\n",
    "    2. 嵌入假设答案：\n",
    "        将假设答案转换为向量（如用 OpenAI embeddings）。\n",
    "    3. 向量检索：\n",
    "        用该向量在数据库中检索真实相关的文档。\n",
    "    4. 生成最终答案：\n",
    "        结合检索到的真实文档生成可靠回答。\n",
    "\"\"\"\n",
    "\n",
    "# 1.文件路径\n",
    "RESOURCE_DIR = \"/mnt/c/大模型/智泊大模型全栈教程总结/02-教材整理 L2/代码/Langchain/6.langchain高级RAG/data/resources\"\n",
    "TXT_DOCUMENT_PATH = os.path.join(RESOURCE_DIR, \"deepseek百度百科.txt\")\n",
    "\n",
    "# 2.模型初始化\n",
    "llm = ChatTongyi(model=\"qwen-max\")\n",
    "embeddings_model = DashScopeEmbeddings(model=\"text-embedding-v1\")\n",
    "\n",
    "# 3.加载文档\n",
    "loader = TextLoader(TXT_DOCUMENT_PATH, encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "# 4.文档向量化并存储\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=chunks,\n",
    "                                    embedding=embeddings_model,\n",
    "                                    collection_name=\"multi-query\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 4.生成假设答案\n",
    "hyde_prompt = \"\"\"\n",
    "    请根据问题生成一个假设性答案（无需准确）：\n",
    "    问题: {question}\n",
    "    假设答案:\n",
    "    \"\"\"\n",
    "question = \"详细介绍DeepSeek\"\n",
    "hypothetical_answer = llm.invoke(hyde_prompt.format(question=question))\n",
    "print(f\"假设答案：\\n {hypothetical_answer.content}\")\n",
    "\n",
    "# 5. 基于答案检索文档\n",
    "retrieved_docs = vectorstore.similarity_search(hypothetical_answer.content, k=3)\n",
    "print(f\"召回文档：\\n {retrieved_docs}\")\n",
    "\n",
    "# 6. 基于召回文档生成答案\n",
    "final_prompt = f\"\"\"\n",
    "    基于以下真实文档回答问题：\n",
    "    文档: {retrieved_docs}\n",
    "    问题: {question}\n",
    "    答案:\n",
    "    \"\"\"\n",
    "print(f\"最终提示：\\n{final_prompt}\")\n",
    "result = llm.invoke(final_prompt)\n",
    "print('-' * 20 + '基于召回文档生成答案' + '-' * 20)\n",
    "print(result)"
   ],
   "id": "1c5823c56bb5591b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 6.混合召回\n",
    "import os\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.dashscope import DashScopeEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i + 1}:\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# 1.文件路径\n",
    "RESOURCE_DIR = \"/mnt/c/大模型/智泊大模型全栈教程总结/02-教材整理 L2/代码/Langchain/6.langchain高级RAG/data/resources\"\n",
    "TXT_DOCUMENT_PATH = os.path.join(RESOURCE_DIR, \"deepseek百度百科.txt\")\n",
    "\n",
    "# 2.模型准备\n",
    "embeddings_model = DashScopeEmbeddings(model=\"text-embedding-v1\")\n",
    "\n",
    "# 3.数据加载\n",
    "loader = TextLoader(TXT_DOCUMENT_PATH, encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "# 4.文档向量化并存储\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks, embedding=embeddings_model, collection_name=\"mix\"\n",
    ")\n",
    "\n",
    "# 5.文档召回\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "query = \"相关评价\"\n",
    "vector_retriever_doc = vector_retriever.invoke(query)\n",
    "print(\"\\n\\n向量召回文档：\" + \"==\" * 100)\n",
    "pretty_print_docs(vector_retriever_doc)\n",
    "\n",
    "# 6.BM25检索\n",
    "BM25_retriever = BM25Retriever.from_documents(chunks, k=3)\n",
    "BM25Retriever_doc = BM25_retriever.invoke(query)\n",
    "print(\"\\n\\n关键词召回文档：\" + \"==\" * 100)\n",
    "pretty_print_docs(BM25Retriever_doc)\n",
    "\n",
    "# 7.混合检索\n",
    "# 向量检索和关键词检索的权重各0.5，两者赋予相同的权重\n",
    "retriever = EnsembleRetriever(retrievers=[BM25_retriever, vector_retriever], weights=[0.5, 0.5])\n",
    "print(\"\\n\\n混合召回文档：\" + \"==\" * 100)\n",
    "pretty_print_docs(retriever.invoke(query))"
   ],
   "id": "edb9b67d7b47d647"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
